#!/usr/bin/env python3
"""
Master Pipeline: Complete CASP Predictions Analysis
Consolidates all steps: cleanup, GARN conversion, RMSD, TM Score, summaries, reports, LaTeX tables
Run this script once to regenerate everything from scratch.
"""

import subprocess
import shutil
from pathlib import Path
import csv
from collections import defaultdict
import pandas as pd
import sys
import os
import concurrent.futures
import argparse

# Configuration
BASE_DIR = Path("/Users/jhonatan/Downloads/casp-predictions")
GARN3_JAR = BASE_DIR / "GARN3.jar"
REFERENCE_DIR = BASE_DIR / "REFERENCE_PBD"

SOURCE_TECHNIQUES = {
    'AlphaFold_GARN': 'AlphaFold',
    'FARFAR2_GARN': 'FARFAR2',
    'FebRNA_GARN': 'FebRNA',
    'iFoldRNA_GARN': 'iFoldRNA',
    'MC-Sym_GARN': 'MC-Sym',
    'NAST_GARN': 'NAST',
    'RNAComposer_GARN': 'RNAComposer',
    'SimRNA_GARN': 'SimRNA',
    'Vfold_GARN': 'Vfold',
    'trRosettaRNA_GARN': 'trRosettaRNA',
    'x3dRNA_GARN': 'x3dRNA',
}

TECHNIQUES = [
    'AlphaFold_GARN',
    'FARFAR2_GARN',
    'FebRNA_GARN',
    'GARN2',
    'GARN3_GARN',
    'iFoldRNA_GARN',
    'MC-Sym_GARN',
    'NAST_GARN',
    'RNAComposer_GARN',
    'SimRNA_GARN',
    'Vfold_GARN',
    'trRosettaRNA_GARN',
    'x3dRNA_GARN',
]

# LaTeX table configurations
LATEX_CONFIGS = [
    {
        'filename': 'rmsd_table.tex',
        'techniques': ['iFoldRNA_GARN', 'NAST_GARN', 'SimRNA_GARN', 'GARN3_GARN'],
        'title': 'Template-Free Methods'
    },
    {
        'filename': 'rmsd_table2.tex',
        'techniques': ['FARFAR2_GARN', 'RNAComposer_GARN', 'x3dRNA_GARN', 'FebRNA_GARN', 'GARN3_GARN'],
        'title': 'Template-Based Methods (Subset)'
    },
    {
        'filename': 'rmsd_table3.tex',
        'techniques': ['FARFAR2_GARN', 'RNAComposer_GARN', 'Vfold_GARN', 'x3dRNA_GARN', 'FebRNA_GARN', 'MC-Sym_GARN', 'GARN3_GARN'],
        'title': 'All Template-Based Methods'
    },
    {
        'filename': 'rmsd_table4.tex',
        'techniques': ['AlphaFold_GARN', 'trRosettaRNA_GARN', 'GARN3_GARN'],
        'title': 'Deep Learning Methods'
    }
]

TECH_LABELS = {
    'AlphaFold_GARN': 'AlphaFold',
    'FARFAR2_GARN': 'FARFAR2',
    'FebRNA_GARN': 'FebRNA',
    'GARN3_GARN': 'GARN3',
    'iFoldRNA_GARN': 'iFoldRNA',
    'MC-Sym_GARN': 'MC-Sym',
    'NAST_GARN': 'NAST',
    'RNAComposer_GARN': 'RNAComposer',
    'SimRNA_GARN': 'SimRNA',
    'Vfold_GARN': 'Vfold',
    'trRosettaRNA_GARN': 'trRosettaRNA',
    'x3dRNA_GARN': 'x3dRNA',
}

def step(msg):
    """Print step header"""
    print(f"\n{'='*70}")
    print(f"{msg}")
    print(f"{'='*70}\n")

def cleanup_generated_files(skip_pdbtogarn: bool):
    """Clean up all generated files and folders"""
    step("STEP 1: CLEANUP")

    # Remove metric and summary files
    patterns = ['*_rmsd.csv', '*_tmscore.csv', 'rmsd_summary.csv', 'tm_score_summary.csv',
                'rmsd_pivot*.csv', 'tm_score_pivot*.csv', '*_by_molecule*.csv', '*_by_molecule*.xlsx',
                'rmsd_summary.xlsx', 'tm_score_summary.xlsx', 'rmsd_table*.tex']
    
    for pattern in patterns:
        for file in BASE_DIR.glob(pattern):
            if file.is_file():
                file.unlink()
                print(f"  Removed {file.name}")

    remove_listnucleo_files()

    if not skip_pdbtogarn:
        # Remove generated *_GARN technique outputs (keep GARN2/GARN3_GARN sources)
        for tech in SOURCE_TECHNIQUES.keys():
            tech_dir = BASE_DIR / tech
            if tech_dir.exists():
                shutil.rmtree(tech_dir)

    # Remove per-technique generated metric files but keep source predictions
    for tech in TECHNIQUES:
        tech_dir = BASE_DIR / tech
        if not tech_dir.exists():
            continue
        for file in tech_dir.rglob('*_rmsd.csv'):
            file.unlink()
        for file in tech_dir.rglob('*_tmscore.csv'):
            file.unlink()

    
    print("✅ Cleanup complete")

def remove_listnucleo_files():
    """Remove listNucleo files generated by JAR in root and technique folders"""
    for file in BASE_DIR.glob('*_listNucleo.csv'):
        if file.is_file():
            file.unlink()
    listnucleo_dirs = [
        *SOURCE_TECHNIQUES.values(),
        *SOURCE_TECHNIQUES.keys(),
        'GARN3_GARN',
    ]
    for dir_name in listnucleo_dirs:
        dir_path = BASE_DIR / dir_name
        if dir_path.exists():
            for file in dir_path.rglob('*_listNucleo.csv'):
                file.unlink()

def ensure_inputs_from_git():
    """Restore required input folders only if missing (do not overwrite local changes)"""
    step("STEP 2: ENSURE INPUTS")

    required_dirs = [
        "PDB",
        "SECONDARY",
        "REFERENCE_PBD",
        *SOURCE_TECHNIQUES.values(),
    ]

    restored = 0
    for dir_name in required_dirs:
        path = BASE_DIR / dir_name
        if path.exists():
            continue

        result = subprocess.run(
            ['git', 'checkout', 'HEAD', '--', dir_name],
            cwd=str(BASE_DIR),
            capture_output=True,
            text=True
        )

        if result.returncode == 0 and path.exists():
            restored += 1
            print(f"  Restored {dir_name}")
        else:
            print(f"  ⚠️ Unable to restore {dir_name}")

    if restored == 0:
        print("  No missing inputs to restore")

def convert_pdb_to_garn():
    """Convert PDB files to GARN format using GARN3.jar"""
    step("STEP 3: CONVERT PDB TO GARN")
    
    PDB_DIR = BASE_DIR / "PDB"
    SECONDARY_DIR = BASE_DIR / "SECONDARY"
    
    if not PDB_DIR.exists() or not SECONDARY_DIR.exists():
        print("⚠️ PDB or SECONDARY directories not found, skipping conversion")
        return
    
    total_converted = 0
    total_failed = 0
    total_missing_pdb = 0
    total_skipped = 0

    max_workers = min(8, (os.cpu_count() or 1))

    def run_pdbtogarn(mol_id, sec_file, pdb_file, garn_path):
        cmd = [
            'java', '-jar', str(GARN3_JAR),
            'PDBTOGARN',
            mol_id,
            str(sec_file),
            pdb_file.name
        ]

        try:
            result = subprocess.run(
                cmd,
                cwd=str(pdb_file.parent),
                capture_output=True,
                timeout=60,
                text=True
            )

            if result.returncode == 0:
                output_candidate = pdb_file.parent / f"{pdb_file.stem}_GARN.csv"
                legacy_output = BASE_DIR / f'{mol_id}_GARN.csv'

                if output_candidate.exists():
                    output_candidate.rename(garn_path)
                    return "converted"
                if legacy_output.exists():
                    legacy_output.rename(garn_path)
                    return "converted"
                return "failed"
            return "failed"
        except Exception:
            return "failed"

    for target_tech, source_tech in SOURCE_TECHNIQUES.items():
        source_dir = BASE_DIR / source_tech
        target_dir = BASE_DIR / target_tech

        if not source_dir.exists():
            continue

        target_dir.mkdir(parents=True, exist_ok=True)

        mol_folders = sorted([d for d in source_dir.iterdir() if d.is_dir()])
        tech_converted = 0
        tech_failed = 0
        tech_skipped = 0

        futures = []
        for mol_dir in mol_folders:
            mol_id = mol_dir.name.upper()
            target_mol_dir = target_dir / mol_dir.name
            target_mol_dir.mkdir(parents=True, exist_ok=True)

            # Get secondary structure file
            sec_file = SECONDARY_DIR / f"{mol_id.lower()}.txt"
            if not sec_file.exists():
                tech_failed += 1
                continue

            pdb_files = sorted(mol_dir.glob("*.pdb"))
            if not pdb_files:
                tech_skipped += 1
                total_skipped += 1
                total_missing_pdb += 1
                continue

            for enum_idx, pdb_file in enumerate(pdb_files, start=1):
                stem = pdb_file.stem
                idx = None
                if stem.startswith(f"GARN_{mol_id}_"):
                    try:
                        idx = int(stem.split('_')[-1])
                    except ValueError:
                        idx = None
                if idx is None:
                    idx = enum_idx
                garn_filename = f"GARN_{mol_id}_{idx}.csv"
                garn_path = target_mol_dir / garn_filename

                # Skip if already converted
                if garn_path.exists():
                    tech_skipped += 1
                    total_skipped += 1
                    continue

                futures.append((mol_id, sec_file, pdb_file, garn_path))

        if futures:
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_map = [executor.submit(run_pdbtogarn, *args) for args in futures]
                for f in concurrent.futures.as_completed(future_map):
                    result = f.result()
                    if result == "converted":
                        tech_converted += 1
                    else:
                        tech_failed += 1

        if tech_converted > 0 or tech_failed > 0 or tech_skipped > 0:
            skipped_note = f", {tech_skipped} skipped" if tech_skipped > 0 else ""
            print(f"  {target_tech}: {tech_converted} converted, {tech_failed} failed{skipped_note}")

        total_converted += tech_converted
        total_failed += tech_failed

    print(f"✅ Total {total_converted} PDB files converted ({total_failed} failed)")
    if total_missing_pdb > 0:
        print(f"↪️  {total_missing_pdb} molecule folders had no .pdb files (skipped)")

    # Remove any listNucleo files produced during conversion
    remove_listnucleo_files()

def normalize_garn_files():
    """Rename all GARN files to unified nomenclature: GARN_{MOL}_{IDX}.csv"""
    step("STEP 4: NORMALIZE GARN FILE NAMES")
    
    total_renamed = 0
    
    for technique_name in TECHNIQUES:
        if technique_name in SOURCE_TECHNIQUES:
            continue
        technique_dir = BASE_DIR / technique_name
        if not technique_dir.exists():
            continue
        
        mol_folders = sorted([d for d in technique_dir.iterdir() if d.is_dir()])
        tech_renamed = 0
        
        for mol_dir in mol_folders:
            mol_id = mol_dir.name.upper()
            # Get all *_GARN.csv files but exclude {mol_id}_GARN.csv (which is reference/summary)
            garn_files = sorted([f for f in mol_dir.glob("*_GARN.csv") if f.name != f"{mol_id}_GARN.csv"])
            
            if not garn_files:
                continue

            start_index = 0 if technique_name == "GARN3_GARN" else 1
            for offset, garn_file in enumerate(garn_files, start_index):
                new_name = f"GARN_{mol_id}_{offset}.csv"
                new_path = mol_dir / new_name
                
                if garn_file != new_path:
                    garn_file.rename(new_path)
                    tech_renamed += 1
                    total_renamed += 1
        
        if tech_renamed > 0:
            print(f"  {technique_name}: {tech_renamed} files renamed")
    
    # Remove auxiliary files generated by JAR (only from technique folders, not REFERENCE_PBD)
    # Keep listNucleo in GARN2 since they are not generated by this script.
    for technique_name in TECHNIQUES:
        if technique_name == "GARN2":
            continue
        technique_dir = BASE_DIR / technique_name
        if technique_dir.exists():
            for f in technique_dir.rglob("*_listNucleo.csv"):
                f.unlink()
    
    print(f"✅ Total {total_renamed} files normalized")

def calculate_rmsd():
    """Calculate RMSD for all techniques"""
    step("STEP 5: CALCULATE RMSD")
    
    total_success = 0
    total_failed = 0
    total_skipped_ref = 0
    total_skipped_pred = 0
    
    for technique_name in TECHNIQUES:
        technique_dir = BASE_DIR / technique_name
        
        if not technique_dir.exists():
            continue
        
        mol_folders = sorted([d for d in technique_dir.iterdir() if d.is_dir()])
        success = 0
        failed = 0
        skipped_ref = 0
        skipped_pred = 0
        
        for mol_dir in mol_folders:
            mol_id = mol_dir.name.upper()
            
            if technique_name == "GARN2":
                ref_file = mol_dir / f"{mol_id}_GARN.csv"
            else:
                ref_file = REFERENCE_DIR / mol_id / f"{mol_id}_GARN.csv"
            
            if not ref_file.exists():
                print(f"  {technique_name}: ⚠️  {mol_id} missing reference, skipping")
                skipped_ref += 1
                continue
            
            predicted_files = sorted([f for f in mol_dir.glob(f"GARN_{mol_id}_*.csv")
                                    if not f.name.endswith(('_rmsd.csv', '_tmscore.csv'))])
            
            if not predicted_files:
                print(f"  {technique_name}: ⚠️  {mol_id} missing predictions, skipping")
                skipped_pred += 1
                continue
            
            cmd = ['java', '-jar', str(GARN3_JAR), 'RMSD', mol_id, str(ref_file)] + \
                  [str(p) for p in predicted_files]
            
            try:
                result = subprocess.run(cmd, cwd=str(BASE_DIR), capture_output=True, timeout=300, text=True)
                
                if result.returncode == 0:
                    rmsd_file = BASE_DIR / f'{mol_id}_rmsd.csv'
                    if rmsd_file.exists():
                        dest = mol_dir / f'GARN_{mol_id}_rmsd.csv'
                        shutil.move(str(rmsd_file), str(dest))
                        success += 1
                    else:
                        failed += 1
                else:
                    failed += 1
            except Exception:
                failed += 1
        
        if success > 0:
            print(f"  {technique_name}: ✅ {success} RMSD")
        if failed > 0:
            print(f"  {technique_name}: ❌ {failed} failed")
        if skipped_ref > 0:
            print(f"  {technique_name}: ↪️  {skipped_ref} skipped (missing reference)")
        if skipped_pred > 0:
            print(f"  {technique_name}: ↪️  {skipped_pred} skipped (missing predictions)")
        
        total_success += success
        total_failed += failed
        total_skipped_ref += skipped_ref
        total_skipped_pred += skipped_pred
    
    print(
        f"✅ Total: {total_success} RMSD calculated, {total_failed} failed, "
        f"{total_skipped_ref} skipped (missing reference), {total_skipped_pred} skipped (missing predictions)"
    )

def calculate_tm_score():
    """Calculate TM Score for all techniques"""
    step("STEP 6: CALCULATE TM SCORE")
    
    total_success = 0
    total_failed = 0
    total_skipped_ref = 0
    total_skipped_pred = 0
    
    for technique_name in TECHNIQUES:
        technique_dir = BASE_DIR / technique_name
        
        if not technique_dir.exists():
            continue
        
        mol_folders = sorted([d for d in technique_dir.iterdir() if d.is_dir()])
        success = 0
        failed = 0
        skipped_ref = 0
        skipped_pred = 0
        
        for mol_dir in mol_folders:
            mol = mol_dir.name.upper()
            
            if technique_name == "GARN2":
                ref_garn = mol_dir / f"{mol}_GARN.csv"
            else:
                ref_garn = REFERENCE_DIR / mol / f"{mol}_GARN.csv"
            
            if not ref_garn.exists():
                print(f"  {technique_name}: ⚠️  {mol} missing reference, skipping")
                skipped_ref += 1
                continue
            
            all_files = sorted(mol_dir.glob(f'GARN_{mol}_*.csv'))
            pred_garns = [f for f in all_files if not f.name.endswith(('_rmsd.csv', '_tmscore.csv'))]
            
            if not pred_garns:
                print(f"  {technique_name}: ⚠️  {mol} missing predictions, skipping")
                skipped_pred += 1
                continue
            
            cmd = ['java', '-jar', str(GARN3_JAR), 'TMSCORE', mol, str(ref_garn)] + \
                  [str(f) for f in pred_garns]
            
            try:
                result = subprocess.run(cmd, cwd=str(BASE_DIR), capture_output=True, timeout=300, text=True)
                
                if result.returncode != 0:
                    failed += 1
                    continue
                    
            except Exception:
                failed += 1
                continue
            
            tm_file = BASE_DIR / f'{mol}_tmscore.csv'
            if tm_file.exists():
                dest = mol_dir / f'GARN_{mol}_tmscore.csv'
                try:
                    shutil.move(str(tm_file), str(dest))
                    success += 1
                except Exception:
                    failed += 1
            else:
                failed += 1
        
        if success > 0:
            print(f"  {technique_name}: ✅ {success} TM Score")
        if failed > 0:
            print(f"  {technique_name}: ⚠️ {failed} failed")
        if skipped_ref > 0:
            print(f"  {technique_name}: ↪️  {skipped_ref} skipped (missing reference)")
        if skipped_pred > 0:
            print(f"  {technique_name}: ↪️  {skipped_pred} skipped (missing predictions)")
        
        total_success += success
        total_failed += failed
        total_skipped_ref += skipped_ref
        total_skipped_pred += skipped_pred
    
    print(
        f"✅ Total: {total_success} TM Score calculated, {total_failed} failed, "
        f"{total_skipped_ref} skipped (missing reference), {total_skipped_pred} skipped (missing predictions)"
    )

def generate_summaries():
    """Generate RMSD and TM Score summary files"""
    step("STEP 7: GENERATE SUMMARIES")
    
    # RMSD Summary
    rmsd_rows = []
    for tech_dir in sorted(BASE_DIR.glob('*_GARN')) + ([BASE_DIR / 'GARN2'] if (BASE_DIR / 'GARN2').exists() else []):
        if not tech_dir.is_dir():
            continue
        
        technique = tech_dir.name
        for mol_dir in sorted(tech_dir.iterdir()):
            if not mol_dir.is_dir():
                continue
            
            mol = mol_dir.name
            rmsd_file = mol_dir / f"GARN_{mol}_rmsd.csv"
            
            if not rmsd_file.exists():
                rmsd_rows.append((technique, mol, "", "", 0, ""))
                continue
            
            values = []
            try:
                with rmsd_file.open() as fh:
                    for line in fh:
                        line = line.strip()
                        if not line:
                            continue
                        parts = line.split(';')
                        if len(parts) >= 2:
                            try:
                                val = float(parts[1])
                                if not pd.isna(val):
                                    values.append(val)
                            except (ValueError, IndexError):
                                pass
            except Exception:
                rmsd_rows.append((technique, mol, "", "", 0, ""))
                continue
            
            if values:
                min_val = min(values)
                max_val = max(values)
                rmsd_rows.append((technique, mol, f"{min_val:.2f}", f"{max_val:.2f}", len(values), "GARN_output"))
            else:
                rmsd_rows.append((technique, mol, "", "", 0, ""))
    
    with open(BASE_DIR / 'rmsd_summary.csv', 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['technique', 'molecule', 'min_rmsd', 'max_rmsd', 'count', 'source'])
        writer.writerows(rmsd_rows)
    
    print(f"  RMSD summary: {len(rmsd_rows)} rows")
    
    # TM Score Summary
    tm_rows = []
    for tm_file in BASE_DIR.rglob('GARN_*_tmscore.csv'):
        if 'REFERENCE_PBD' in str(tm_file):
            continue
        
        parts = tm_file.relative_to(BASE_DIR).parts
        if len(parts) < 2:
            continue
        
        tech_dir = parts[0]
        mol = parts[1]
        tech = tech_dir
        
        values = []
        try:
            with tm_file.open() as fh:
                for line in fh:
                    line = line.strip()
                    if not line:
                        continue
                    parts_split = line.split(';')
                    if len(parts_split) >= 2:
                        try:
                            val = float(parts_split[1])
                            if not pd.isna(val):
                                values.append(val)
                        except (ValueError, IndexError):
                            pass
        except Exception:
            continue
        
        if values:
            min_tm = min(values)
            max_tm = max(values)
            tm_rows.append((tech, mol, f"{min_tm:.4f}", f"{max_tm:.4f}", len(values)))
    
    with open(BASE_DIR / 'tm_score_summary.csv', 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['technique', 'molecule', 'min_tm_score', 'max_tm_score', 'count'])
        writer.writerows(tm_rows)
    
    print(f"  TM Score summary: {len(tm_rows)} rows")
    print("✅ Summaries generated")

def generate_reports():
    """Generate Excel reports and pivot tables"""
    step("STEP 8: GENERATE REPORTS")
    
    # RMSD Report
    rmsd_df = pd.read_csv(BASE_DIR / 'rmsd_summary.csv')
    
    if not rmsd_df.empty:
        rmsd_pivot_min = rmsd_df.pivot_table(
            values='min_rmsd', index='molecule', columns='technique',
            aggfunc='first'
        )
        rmsd_pivot_min.to_csv(BASE_DIR / 'rmsd_pivot_min.csv')
        
        rmsd_pivot_max = rmsd_df.pivot_table(
            values='max_rmsd', index='molecule', columns='technique',
            aggfunc='first'
        )
        rmsd_pivot_max.to_csv(BASE_DIR / 'rmsd_pivot_max.csv')
        
        with pd.ExcelWriter(BASE_DIR / 'rmsd_summary.xlsx') as writer:
            rmsd_df.to_excel(writer, sheet_name='Summary', index=False)
            rmsd_pivot_min.to_excel(writer, sheet_name='Min RMSD')
            rmsd_pivot_max.to_excel(writer, sheet_name='Max RMSD')
        
        print("  RMSD reports: ✅")
    
    # TM Score Report
    tm_df = pd.read_csv(BASE_DIR / 'tm_score_summary.csv')
    
    if not tm_df.empty:
        tm_pivot_min = tm_df.pivot_table(
            values='min_tm_score', index='molecule', columns='technique',
            aggfunc='first'
        )
        tm_pivot_min.to_csv(BASE_DIR / 'tm_score_pivot_min.csv')
        
        tm_pivot_max = tm_df.pivot_table(
            values='max_tm_score', index='molecule', columns='technique',
            aggfunc='first'
        )
        tm_pivot_max.to_csv(BASE_DIR / 'tm_score_pivot_max.csv')
        
        with pd.ExcelWriter(BASE_DIR / 'tm_score_summary.xlsx') as writer:
            tm_df.to_excel(writer, sheet_name='Summary', index=False)
            tm_pivot_min.to_excel(writer, sheet_name='Min TM Score')
            tm_pivot_max.to_excel(writer, sheet_name='Max TM Score')
        
        print("  TM Score reports: ✅")
    
    print("✅ Reports generated")

def generate_latex_tables():
    """Generate LaTeX tables with combined RMSD/TM Score format"""
    step("STEP 9: GENERATE LATEX TABLES")

    rmsd_df = pd.read_csv(BASE_DIR / 'rmsd_summary.csv')
    tm_df = pd.read_csv(BASE_DIR / 'tm_score_summary.csv')
    mol_sizes_path = BASE_DIR / 'mol_sizes.csv'

    mol_info = {}
    if mol_sizes_path.exists():
        sizes_df = pd.read_csv(mol_sizes_path, sep=';')
        for _, row in sizes_df.iterrows():
            mol = str(row.get('mol', '')).strip().upper()
            if not mol:
                continue
            mol_info[mol] = {
                'type': str(row.get('topology', '')).strip(),
                'length': str(row.get('length', '')).strip(),
            }

    def format_type(value: str) -> str:
        if not value:
            return '–'
        val = value.strip().lower()
        if 'pseudo' in val and 'knot' in val:
            return "\\begin{tabular}[c]{@{}l@{}}pseudo-\\nknot\\end{tabular}"
        if '2-way' in val:
            return '2-way'
        if '3-way' in val:
            return '3-way'
        if 'n-way' in val:
            return 'n-way'
        return value

    for config in LATEX_CONFIGS:
        techniques_in_config = config['techniques']

        rmsd_filtered = rmsd_df[rmsd_df['technique'].isin(techniques_in_config)]
        tm_filtered = tm_df[tm_df['technique'].isin(techniques_in_config)]

        molecules = sorted(set(list(rmsd_filtered['molecule'].unique()) +
                             list(tm_filtered['molecule'].unique())))

        latex_lines = [
            "\\begin{table}[!ht]",
            "\\begin{adjustwidth}{-2.5in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.",
            "\\centering",
            f"\\caption{{\\textbf{{{config['title']}}}}}",
            "\\begin{tabular}{lllllllllll}",
            "\\toprule",
            "\\textbf{Molecule} & \\textbf{Type} & \\textbf{Length} & \\textbf{RMSD / TM} & " +
            " & ".join(f"\\textbf{{{TECH_LABELS.get(t, t)}}}" for t in techniques_in_config) + " \\",
            "\\midrule",
        ]

        for mol in molecules:
            mol_key = str(mol).upper()
            info = mol_info.get(mol_key, {})
            mol_type = format_type(info.get('type', '–'))
            mol_len = info.get('length', '–') or '–'

            rmsd_row = rmsd_filtered[rmsd_filtered['molecule'] == mol]
            tm_row = tm_filtered[tm_filtered['molecule'] == mol]

            def is_valid_number(value):
                if value is None:
                    return False
                try:
                    if pd.isna(value):
                        return False
                except Exception:
                    pass
                val_str = str(value).strip().lower()
                if val_str in {'', 'nan', 'none'}:
                    return False
                return True

            min_rmsd_vals = []
            max_rmsd_vals = []
            max_tm_vals = []
            min_tm_vals = []

            for tech in techniques_in_config:
                r = rmsd_row[rmsd_row['technique'] == tech]
                t = tm_row[tm_row['technique'] == tech]
                if len(r) > 0 and is_valid_number(r['min_rmsd'].values[0]):
                    try:
                        min_rmsd_vals.append(float(r['min_rmsd'].values[0]))
                    except ValueError:
                        pass
                if len(r) > 0 and is_valid_number(r['max_rmsd'].values[0]):
                    try:
                        max_rmsd_vals.append(float(r['max_rmsd'].values[0]))
                    except ValueError:
                        pass
                if len(t) > 0 and is_valid_number(t['max_tm_score'].values[0]):
                    try:
                        max_tm_vals.append(float(t['max_tm_score'].values[0]))
                    except ValueError:
                        pass
                if len(t) > 0 and is_valid_number(t['min_tm_score'].values[0]):
                    try:
                        min_tm_vals.append(float(t['min_tm_score'].values[0]))
                    except ValueError:
                        pass

            best_min_rmsd = min(min_rmsd_vals) if min_rmsd_vals else None
            best_max_rmsd = min(max_rmsd_vals) if max_rmsd_vals else None
            best_max_tm = max(max_tm_vals) if max_tm_vals else None
            best_min_tm = max(min_tm_vals) if min_tm_vals else None

            min_line = f"\\multirow{{2}}{{*}}{{{mol}}} & \\multirow{{2}}{{*}}{{{mol_type}}} & \\multirow{{2}}{{*}}{{{mol_len}}} & Min"
            max_line = " &  &  & Max"

            for tech in techniques_in_config:
                r = rmsd_row[rmsd_row['technique'] == tech]
                t = tm_row[tm_row['technique'] == tech]

                min_rmsd = r['min_rmsd'].values[0] if len(r) > 0 and is_valid_number(r['min_rmsd'].values[0]) else ''
                max_rmsd = r['max_rmsd'].values[0] if len(r) > 0 and is_valid_number(r['max_rmsd'].values[0]) else ''
                max_tm = t['max_tm_score'].values[0] if len(t) > 0 and is_valid_number(t['max_tm_score'].values[0]) else ''
                min_tm = t['min_tm_score'].values[0] if len(t) > 0 and is_valid_number(t['min_tm_score'].values[0]) else ''

                if min_rmsd and best_min_rmsd is not None:
                    try:
                        if float(min_rmsd) == best_min_rmsd:
                            min_rmsd = f"\\textbf{{{min_rmsd}}}"
                    except ValueError:
                        pass
                if max_tm and best_max_tm is not None:
                    try:
                        if float(max_tm) == best_max_tm:
                            max_tm = f"\\textbf{{{max_tm}}}"
                    except ValueError:
                        pass
                if max_rmsd and best_max_rmsd is not None:
                    try:
                        if float(max_rmsd) == best_max_rmsd:
                            max_rmsd = f"\\textbf{{{max_rmsd}}}"
                    except ValueError:
                        pass
                if min_tm and best_min_tm is not None:
                    try:
                        if float(min_tm) == best_min_tm:
                            min_tm = f"\\textbf{{{min_tm}}}"
                    except ValueError:
                        pass

                if min_rmsd and max_tm:
                    min_line += f" & {min_rmsd} / {max_tm}"
                elif min_rmsd:
                    min_line += f" & {min_rmsd}"
                elif max_tm:
                    min_line += f" & / {max_tm}"
                else:
                    min_line += " & –"

                if max_rmsd and min_tm:
                    max_line += f" & {max_rmsd} / {min_tm}"
                elif max_rmsd:
                    max_line += f" & {max_rmsd}"
                elif min_tm:
                    max_line += f" & / {min_tm}"
                else:
                    max_line += " & –"

            latex_lines.append(min_line + " \\")
            latex_lines.append(max_line + " \\")
            latex_lines.append("\\hline")

        latex_lines.extend([
            "\\bottomrule",
            "\\end{tabular}",
            "\\end{adjustwidth}",
            "\\end{table}",
        ])

        with open(BASE_DIR / config['filename'], 'w') as f:
            f.write('\n'.join(latex_lines))

        print(f"  {config['filename']}: ✅ ({len(molecules)} molecules)")

    print("✅ LaTeX tables generated")

def main():
    """Run complete pipeline"""
    parser = argparse.ArgumentParser(description="Run CASP predictions pipeline")
    parser.add_argument(
        "--skip-pdbtogarn",
        action="store_true",
        help="Skip PDBTOGARN generation and only clean metrics outputs",
    )
    args = parser.parse_args()

    print("\n" + "="*70)
    print("MASTER PIPELINE: Complete CASP Predictions Analysis")
    print("="*70)
    
    try:
        cleanup_generated_files(skip_pdbtogarn=args.skip_pdbtogarn)
        ensure_inputs_from_git()
        if not args.skip_pdbtogarn:
            convert_pdb_to_garn()
        normalize_garn_files()
        calculate_rmsd()
        calculate_tm_score()
        generate_summaries()
        generate_reports()
        generate_latex_tables()
        
        print("\n" + "="*70)
        print("✅ PIPELINE COMPLETE!")
        print("="*70 + "\n")
        
    except Exception as e:
        print(f"\n❌ ERROR: {e}\n")
        sys.exit(1)

if __name__ == '__main__':
    main()
